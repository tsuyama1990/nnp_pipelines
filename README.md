# ACE Active Carver

An automated Active Learning system for materials science, designed to train Atomic Cluster Expansion (ACE) potentials using a Hybrid MD-kMC workflow. The system autonomously explores phase space, identifies high-uncertainty configurations, and retrains the potential using First-Principles (DFT) data.

## Architecture

This project follows a **Micro-kernel Architecture**:

*   **Orchestrator (Host)**: A lightweight Python application that manages the active learning loop, state, and decision logic. It does not perform heavy computations itself.
*   **Workers (Docker Containers)**: Specialized, isolated environments for heavy computational tasks. The Orchestrator invokes these workers via `docker run`.
    *   `gen_worker`: Generates candidate structures using MACE (Foundational ML Force Field) and PyXtal (Symmetry-based generation).
    *   `dft_worker`: Performs First-Principles calculations (Quantum Espresso) to label data.
    *   `pace_worker`: Trains ACE potentials and performs uncertainty-based sampling (Pacemaker).
    *   `lammps_worker`: Runs Molecular Dynamics (MD) and Kinetic Monte Carlo (kMC) simulations (LAMMPS).
*   **Shared Data**: Data is exchanged via a shared volume mounted at `./data` on the host and `/data` inside containers.

## Prerequisites

*   **Linux OS** (Ubuntu/Debian recommended)
*   **Docker Engine** (with non-root user access configured)
*   **NVIDIA Drivers** & **NVIDIA Container Toolkit** (required for GPU acceleration in `pace_worker` and `gen_worker`)
*   **uv** (Python package manager)

## Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-org/ace-active-carver.git
    cd ace-active-carver
    ```

2.  **Install dependencies using `uv`:**
    ```bash
    # Install uv if not present
    curl -LsSf https://astral.sh/uv/install.sh | sh

    # Sync dependencies
    uv sync
    ```

3.  **Build Worker Images:**
    Each worker has its own `Dockerfile` in `workers/<name>`. You must build them before running the orchestrator.
    ```bash
    # Example build script usage (if available) or manual build:
    docker build -t dft_worker:latest -f workers/dft_worker/Dockerfile .
    docker build -t gen_worker:latest -f workers/gen_worker/Dockerfile .
    docker build -t pace_worker:latest -f workers/pace_worker/Dockerfile .
    docker build -t lammps_worker:latest -f workers/lammps_worker/Dockerfile .
    ```

## ğŸš€ Usage Workflow

æœ¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã€å†ç¾æ€§ã¨è¨­å®šç®¡ç†ã®ãŸã‚ã« `setup_experiment.py` ã‚’å”¯ä¸€ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã¨ã—ã¦è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚
ç›´æ¥ `orchestrator/main.py` ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã¯æ¨å¥¨ã•ã‚Œã¾ã›ã‚“ã€‚

### 1. Configuration (è¨­å®š)
å®Ÿé¨“ã®è¨­å®šã¯ `config.yaml` ã§ç®¡ç†ã—ã¾ã™ã€‚
ç›®çš„ã«å¿œã˜ã¦è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ãƒ»ç·¨é›†ã—ã¦ãã ã•ã„ã€‚

```bash
cp config.yaml my_experiment_config.yaml
# vim my_experiment_config.yaml
```

### 2\. Initialize & Run Experiment (å®Ÿè¡Œ)

`setup_experiment.py` ã‚’ä»‹ã—ã¦å®Ÿé¨“ã‚’é–‹å§‹ã—ã¾ã™ã€‚ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’è‡ªå‹•åŒ–ã—ã¾ã™ï¼š

1.  **Workspaceä½œæˆ:** ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå®Ÿé¨“IDã‚’æŒã¤ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆ`experiments/YYYYMMDD_HHMMSS_Name`ï¼‰ã‚’ä½œæˆã€‚
2.  **Configå‡çµ:** ä½¿ç”¨ã—ãŸè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«ã‚³ãƒ”ãƒ¼ï¼ˆå†ç¾æ€§ã®æ‹…ä¿ï¼‰ã€‚
3.  **åˆæœŸåŒ–:** Seedç”Ÿæˆã€åˆæœŸãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã®æº–å‚™ã€‚
4.  **ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³èµ·å‹•:** `ActiveLearningOrchestrator` ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ã€‚

#### åŸºæœ¬ã‚³ãƒãƒ³ãƒ‰

```bash
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å®Ÿè¡Œ
uv run setup_experiment.py

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦å®Ÿè¡Œï¼ˆæ¨å¥¨ï¼‰
uv run setup_experiment.py --config my_experiment_config.yaml

# å®Ÿé¨“åï¼ˆã‚¿ã‚°ï¼‰ã‚’ä»˜ã‘ã¦å®Ÿè¡Œ
uv run setup_experiment.py --config config.yaml --name "al_ni_system_v1"
```

### 3\. Directory Structure (å‡ºåŠ›æ§‹é€ )

å®Ÿè¡Œå¾Œã€ä»¥ä¸‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ãŒè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã™ã€‚

```text
work/
â””â”€â”€ 07_active_learning/          # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ¡ã‚¤ãƒ³ä½œæ¥­é ˜åŸŸ
    â”œâ”€â”€ experiment_state.json    # ä¸­æ–­å†é–‹ç”¨ã®ã‚¹ãƒ†ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
    â”œâ”€â”€ config_snapshot.yaml     # å®Ÿè¡Œæ™‚ã®è¨­å®šï¼ˆå‡çµï¼‰
    â”œâ”€â”€ iteration_1/             # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ã®è¨ˆç®—çµæœ
    â”‚   â”œâ”€â”€ candidate.xyz
    â”‚   â”œâ”€â”€ train.xyz
    â”‚   â””â”€â”€ potential_v1.yace
    â””â”€â”€ logs/
        â””â”€â”€ experiment.log
```

### 4\. Resume / Restart (ä¸­æ–­ã¨å†é–‹)

å®Ÿé¨“ãŒä¸­æ–­ã—ãŸå ´åˆã€ç”Ÿæˆã•ã‚ŒãŸå®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¦å†é–‹ã—ã¾ã™ã€‚

```bash
# ç‰¹å®šã®å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å†é–‹ã™ã‚‹å ´åˆ
uv run setup_experiment.py --resume work/07_active_learning/ --iteration 5
```

## Directory Structure

*   `orchestrator/`: Python code for the control logic.
    *   `src/setup/`: Modules for experiment initialization.
    *   `src/wrappers/`: Docker wrappers that construct CLI commands for workers.
    *   `src/services/`: Business logic for MD, KMC, and Active Learning.
    *   `src/utils/`: Utility classes, including parallel execution helpers.
*   `workers/`: Source code and Dockerfiles for computational workers.
    *   `dft_worker/`: Quantum Espresso wrapper.
    *   `gen_worker/`: MACE structure generation and PyXtal integration.
    *   `pace_worker/`: Pacemaker training and sampling.
    *   `lammps_worker/`: LAMMPS MD/KMC engine.
*   `shared/`: Common Python code (Config, Data Structures) shared between Host and Workers.
*   `data/`: Runtime data directory (mounted to containers).

## License

[Insert License Here]
